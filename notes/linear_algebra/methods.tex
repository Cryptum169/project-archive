\subsubsection{Whitening Transformation}

\paragraph{Introduction}

Otherwise known as \textbf{Sphering Transformation}, is a linear transformation
that transforms a vector of random variables with a known covariance
matrix into a set of new RVs whose covariance is the identity matrix,
\textbf{meaning uncorrelated\footnote{Not necessarily independent} and each have variance 1}.  
Transformation is called whitening because it changes the input vector
to a "white noise vector", zero mean and finite variance, and statistically independent. 

\begin{equation*}
    Cov(X, Y) = 0 \Rightarrow \text{ Uncorrelated vs Independent } P(X, Y) = P(X)P(Y)
\end{equation*}

For column vector $X$ with non-singular covariance matrix $\Sigma$ and mean \textbf{0}, then

\begin{equation*}
    Y = WX, W^TW = \Sigma^{-1}
\end{equation*} 

gives whitened random vector $Y$ with unit diagonal covariance. There are infinitely many possible
whitening matrices $W$. Common coices are

\begin{enumerate}[itemsep=0pt]
    \item $W=\Sigma^{-1/2}$ (Mahalanobis or ZCA whitening)
    \item Cholesky decomposition of $\Sigma^{-1}$
    \item Eigen-system of $\Sigma$
\end{enumerate}

\paragraph{Optimal whitening} transform can be singled out via cross-covariance and cross-correlation of 
$X$ and $Y$. The unique optimal whitening transformation achieving maximal component-wise
correlation between original X and whitened Y is produced by the whitening matrix
\begin{equation*}
    W=P^{-1/2}V^{-1/2}
\end{equation*}
where P is the correlation matrix and V is the variance matrix. 
\textcolor{blue}{
    This seems more of a verification, rather than a method to obtain the result,
    need to check the original citation \cite{optimal_whitening}. 
}

\paragraph{Related transformation}

\begin{itemize}[itemsep=0pt]
    \item Decorrelation transform: removes only the correlations but leaves variances intact
    \item Standardization transform sets variances to 1 but leaves correlations intact
    \item Coloring transformation transforms a vector of white random variables into a random vector with a specified covariance matrix
\end{itemize}

\subsubsection{Cholesky decomposition} \label{sec:cho-decomp}

\paragraph{Introduction} Otherwise known as Cholesky factorization,
is a decomposition of a Hermitian positive-definite matrix to the product
of a lower triangular and its conjugate transpose, which is useful for
efficient numerical solutions, e.g. Monte Carlo simulations
\footnote{When applicable, roughly twice as efficient compared to LU decomposition for
solving linear systems \cite{numerical_c}}.

\paragraph{Statement} The cholesky decomposition of a Hermitian positive-definite matrix
\begin{equation*}
    A = LL^*
\end{equation*}

$L$ is lower triangular matrix with real and positive diagonal entries, and $L^*$
denotes the conjugate tranpose of $L$. Every A has a unique Cholesky decomposition. Converse
holds trivially. Positive semi-definite matrix holds similarly, except the diagonal are
allowed to be zero. 

\paragraph{Relevant Methods: LDL}

\begin{equation*}
    A = LDL^* = LD^{1/2}(D^{1/2})^*L^* = \left(LD^{1/2}\right) \left(LD^{1/2}\right)^*
\end{equation*}

$L$ is a lower triangular matrix, and D is a diagonal matrix. Main advantage being that LDL
decomposition can be computed and used with essentially the same algorith, but \textbf{avoids extracting
square roots}. Therefore often called \emph{square-root-free} Cholesky decomposition.

Otherwise called as LDLT decomposition for real matrices. Closely related to the
eigen decomposition of real symmetric matrices, $\mathbf{A = Q\Lambda Q^T}$,
see section \ref{sec:eigen-decomp}.

\paragraph{Applications} Mainly used for the numerical solution of linear equations 
$\mathbf{Ax = b}$. If $\mathbf{A}$ is symmetric and positive definite, then we can solve
it by computing Cholesky decomposition $\mathbf{A = LL^*}$, then $\mathbf{Ly = b}$ by
forward substitution, and finally $\mathbf{L^*x = y}$ by back substitution.

Alternative way to eliminate taking square roots in the $\mathbf{LL^*}$ is via LDLT decomposition.

\paragraph{Computation}

The cholesky algorithm is a modified version of Gaussian elimination. It recursively computes matrix $A$
in the form that

\begin{equation*}
    A^{(i)} = LA^{(i+1)}L^*
\end{equation*}

Where $A^{(1)} = A$, and at each step, $A^{(i)}$ gives meaning in the following form

\begin{equation*}
    A^{(i)} = \begin{bmatrix}
        I & 0 & 0 \\ 0 & a_{i, i} & \mathbf{b}^* \\ 0 & \mathbf{b} & \mathbf{B}
    \end{bmatrix}, 
    L = \begin{bmatrix}
        I & 0 & 0 \\ 0 & \sqrt{a_{i, i}} & 0 \\ 0 & \frac{1}{\sqrt{a_{i, i}}}\mathbf{b} & I
    \end{bmatrix}
\end{equation*}

We compute recursively in this manner until the following $\mathbf{A}^{i+1}$ matrix
becomes identity matrix.

\begin{equation*}
    A^{i+1} = \begin{bmatrix}
        I & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & \mathbf{B} - \frac{1}{a_{i, i}}\mathbf{b}\mathbf{b}^*
    \end{bmatrix}
\end{equation*}

This operation in total operates in about $O(n^3)$, roughly about $n^3/3$ FLOPs.

\subsubsection{QR Decomposition}

\paragraph{Introduction} Decomposition of a matrix $A$ into a product $A = QR$ of 
orthogonal matrix $Q$ and upper triangular matrix $R$.

\paragraph{Square Matrix} For real square matrices, $A = QR$, if $A$ is invertible, then factorization is
unique if we require the diagonal elements of R to be positive. For n linearly independent columns
in $A$, first n columns of Q form an orthonormal basis for the column space of A. 

\paragraph{Rectangular Matrix} More generally, we factor a complex $m \times n, (m \geq n)$ matrix A.
Product would be $m \times m Q$ and $m \times n$ uppper triangular matrix R. 

\begin{equation*}
    A = QR = Q\begin{bmatrix} R_1 \\ 0 \end{bmatrix}
\end{equation*}

\paragraph{Computation} Gram-Schmidt method, Householder reflection. 
\textcolor{red}{numerical instability and add others, explain how to use householder reflection}

\paragraph{Application} Least squares problem

\begin{equation*}
    \begin{split}
        A^*Ax & = A^*b\\
        R^*Rx = (QR)^*QRx = R^*Q^*b & \Leftrightarrow Rx = Q^*b
    \end{split}
\end{equation*}

There are a couple of advantages to this method
\begin{enumerate}[itemsep=0pt]
    \item Numerical stability ($A^TA$ is worse than $A$, since the ratio of eigen vector is bigger)
    \item Faster
\end{enumerate}

\subsubsection{Eigendecomposition of a Matrix} \label{sec:eigen-decomp}

\paragraph{Introduction} Factorization of a matrix into a canonical form, whereby matrix
is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable\footnote{
    there exists an invertible matrix $P$ and a diagonal matrix $D$ such that $P^{-1}AP=D$
}
matrices can be factorized in this way.

\paragraph{Statement} Let $A$ be square matrix $n \times n$ with $n$ linearly independent
eigenvectors $q_i$, then $A$ can be factorized as 

\begin{equation*}
    A = Q\Lambda Q^{-1}
\end{equation*}

where $Q$ is the square matrix $n \times n$ whoce $i$th column is the $i$th eigenvector of 
$A$ and $\Lambda$ is the diagonal matrix whose diagonal elements are the corresponding
eigenvalues. The decomposition can be derived from the fundamental property of eigenvectors:

\begin{equation*}
    Av = \lambda v \Rightarrow AQ = Q\Lambda \Rightarrow A = Q\Lambda Q^{-1}
\end{equation*}

\paragraph{Other characteristics} Eigendecomposition can be used for example as matrix
inversion.

\begin{equation*}
    \mathbf{A}^{-1} = \mathbf{Q}\mathbf{\Lambda}^{-1}\mathbf{Q}^{-1}
\end{equation*}

If the original matrix is symmetric, then since Q is formed from the eigenvectors, it is
guaranteed to be an orthogonal matrix, therefore $Q^{-1} = Q^T$.


\subsubsection{Gramâ€“Schmidt Process}
\paragraph{Introduction} The GS process is a method for orthonormalizing a set of vectors in an inner product space.
The process takes a finite, linearly independent set $S = \{v_1, \dotsb v_k\}$
and generates an orthogonal set $S{'} = \{u_1, \dotsb, u_k\}, k \leq n$ that
spans the same k-dimensional subspace of $\mathcal{R}^n$ as S.

Application of the GS process to the column vectors of a full column rank matrix 
yields the QR decomposition. 

\paragraph{Computation} Na\"{i}vely, we compute the the set via
$\text{proj}_u(v) = \frac{\langle u, v \rangle}{\langle u, u \rangle}u$
\begin{equation*}
    \begin{split}
        u_1 & = v_1\\
        u_2 & = v_2 - \text{proj}_{u_1}(v_2)\\
        u_3 & = v_3 - \text{proj}_{u_1}(v_3) - \text{proj}_{u_2}(v_3)\\
        \vdots\\
        u_k & = v_k - \sum_{j=1}^{k-1} \text{proj}_{u_j}{v_k}
    \end{split}
\end{equation*}

\paragraph{Numerical Stability} When implemented on a computer, the vectors are
often not quite orthogonal, due to rounding errors. For the classical GS 
process, the loss of orthogonality is particularly bad, therefore it is said
that the classical GFS process is numerically unstable. 

This process \emph{can} be stablized by a small modification, sometimes referred
to as modified GS (MGS). Gives the same result as the original formula and introduces
smaller errors in finite precision arithmetic. 

We do this by iteratively project onto the previous computed vector
\begin{equation*}
    \begin{split}
        u_k^{(1)} = v_k - \text{proj}_{u_1}(v_k)\\
        u_k^{(2)} = u_k^{(1)} - \text{proj}_{u_1}(u_k^{(1)})\\
        \vdots\\
        u_k^{(k-1)} = u_k^{(k-2)} - \text{proj}_{u_{k-1}}(u_k^{k-2})\\
        u_k = \frac{u_k^{(k-1)}}{||u_k^{(k-1)}||}
    \end{split}
\end{equation*}

\paragraph{Alternatives} Other orthogonalization algorithms includes
householder transformation or Givens rotations, or the use of Cholesky decomposition. 

Householder are more stable than the MGS, but Householder produces all vector only at the
end, making GS process applicable for other methods.

\subsubsection{Householder Reflection}
\paragraph{Introduction} A linear transformation that describes a reflection about a 
plane or hyperplane containing the origin. 
